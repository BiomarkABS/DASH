---
title: "Import and QC of OTG Data"
author: 
- name: Mike Ackerman
  email: mike.ackerman@merck.com
- name: Kevin See
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{otg-import-qc}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, results = "hide"}
# knitr options
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = TRUE,
  comment = "#>"
)

```

# Introduction

Welcome to the [DASH](https://github.com/BiomarkABS/DASH) R package! This vignette, which logically follows as the first of multiple vignettes in the package, describes how to import and quality control (QC) on-the-ground (OTG hereafter) collected habitat data  using the DASH protocol. The initial data collection forms to record the OTG data were generated using [ArcGIS Survey123](https://www.esri.com/en-us/arcgis/products/arcgis-survey123/overview) and we focus on importing data from those forms. After data import, we describe how functions included in the DASH R package can then be used to perform QC on the data to identify potential errors, perhaps that occurred in the field. Finally, we describe initial data cleaning and joining (i.e., joining channel unit scale information with wood, undercut, etc. information collected for that unit) in preparation of attaching the OTG collected data to stream centerline data to make it spatial. Later, our spatial OTG data can then be connected to drone-generated orthomosaics. 

Let's start by describing the data import procedure; we'll then go into data QC and how to resolve some of the identified errors. Begin by loading the necessary libraries (R packages) needed for this vignette and analysis:

```{r setup}
# load necessary libraries
library(DASH)
library(tibble)
library(janitor)
library(dplyr)
library(magrittr)

```

# Data Import

Each DASH OTG survey conducted using [ArcGIS Survey123](https://www.esri.com/en-us/arcgis/products/arcgis-survey123/overview) data collection forms and for a site typically consists of 7 comma-delimited (.csv) files:

* **surveyPoint_0.csv**: Information related to the site and survey including site name, survey date/time and crew, and location (lat/long).
* **CU_1.csv**: Data collected for individual channel units. Valid channel units (should) include the following: Riffle, Run, Pool, Rapid+, OCA (off-channel area), and SSC (small side channel).
* **Wood_2.csv**: Data for large wood measured within channel units including count, size, etc.
* **Jam_3.csv**: Data for wood jams within channel units.
* **Undercut_4.csv**: Data for undercuts within channel units.
* **Discharge_5.csv**: Data indicating the location where discharge was measured at a site.
* **DischargeMeasurements_6.csv**: The width, depth, and velocity measurements for each station where discharge is to be estimated.

Each of the .csv files can be joined or related to each other using the **GlobalID** and **ParentGlobalID** columns in each. For example, the **ParentGlobalID** in **CU_1.csv** can be used to related it back to a **GlobalID** in **surveyPoint_0.csv**, whereas the **GlobalID** in **CU_1.csv** is a unique identifier for each channel unit. Similarly, the **ParentGlobalID** in **Wood_2.csv** can be used to join it back to a **GlobalID** in **CU_1.csv**. Note that the **surveyPoint_0.csv** file does not contain a **ParentGlobalID** column as it does not have a "parent" (it is the parent).

To start, let's set `otg_path` a path to the directory containing the ArcGIS Survey123 folder of interest. `otg_path` should only contain folders (no files), and each of those folders should only contain the above files (and no folders).

```{r otg-path}
# As an example, if the Survey123 site folders were in a "1_formatted_csvs" folder on your Desktop
otg_path = "C:/Users/username/Desktop/1_formatted_csvs/"

# For internal Biomark folks, set nas_prefix based on your operating system and then set otg_path to the NAS
if(.Platform$OS.type != 'unix') {
  nas_prefix = "S:"
} else if(.Platform$OS.type == 'unix') {
  nas_prefix = "~/../../Volumes/ABS"
}

# a theoretical path on the NAS
otg_path = paste0(nas_prefix, "/data/habitat/DASH/OTG/1_formatted_csvs/")

```

## `read_otg_csv()`: Reading in One Type of OTG Data at a Time

The [DASH](https://github.com/BiomarkABS/DASH) R package includes a function `read_otg_csv()` which can be used to import one type (`otg_type`) of OTG data at a time. `otg_type` includes two arguments, `path` and `otg_type`. Here, we provide examples of how `read_otg_csv()` can be used to import channel unit (cu) or wood data to create two objects, `otg_raw_cu` and `otg_raw_wood`, respectively. In our case, we'll set `path = otg_path` which we created above, and `otg_type` is set using the name of the files containing the OTG data of interest.

`read_otg_csv()` performs some QC during the data import process. First, `read_otg_csv()` ensures that the column names and specifications in the file being imported matches the expected, which is defined in `get_otg_col_specs()`. If the column names or specifications do not match the expected, `read_otg_csv()` will provide a warning indicating so, but will attempt to proceed. The function also performs one additional QC ensuring that the file being imported contains the expected number of columns and will provide a "fatal error" if not and stop the import process. Throughout the process, you will see which file is being imported and whether the import was successful or not. Finally, `read_otg_csv()` will provide a message if no records are found in a file, which can be totally fine, for example, if a site contained no woody debris, jams, undercuts, etc. In this case, `read_otg_csv()` will just provide a friendly message and proceed.

```{r read-otg-csv, eval = F}
# read in just one type of OTG data at a time
otg_raw_cu = read_otg_csv(path = otg_path,
                          otg_type = "CU_1.csv")

otg_raw_wood = read_otg_csv(path = otg_path,
                            otg_type = "Wood_2.csv")

```

During your initial data import process, you may identify files that aren't formatted correctly for import and `read_otg_csv()` will, rightly so, "choke" during import and indicate so. For example, a .csv file may have inadvertently been saved or exported as tab-delimited, or personnel may have unknowingly added or removed a column. In this case, we suggest storing all of the Survey123 site folders in two separate directories. I have been storing two versions, saved separately in folders "0_raw_csvs" and "1_formatted_csvs". Here, I keep the "raw" untouched data in "0_raw_csvs" which I **never** change. The "1_formatted_csvs" folder is where I make formatting changes that I've identified during the import process. It's typically good practice to always store the rawest form of your data that is never changed after data collection for archiving sake; hence, why we are importing data from a "1_formatted_csvs" folder.

Please note that at any time you may access the help menu and documentation for the package or functions using the following:

```{r help-menu, eval = F}
# for the package
help(package = "DASH")

# for a function
?read_otg_csv

```

## `read_otg_csv_wrapper()`: Read in All OTG Data At Once

It would be a bit tedious to import each of our 7 OTG data types (`otg_type`), separately, and later somehow join them together. To remedy this, we've included the function `read_otg_csv_wrapper()` in the [DASH](https://github.com/BiomarkABS/DASH) R package, which can be used to import some or all of the OTG data types at once. The result is a list of tibbles (i.e., simple data frames) each containing data for an `otg_type`. Rather than having the argument `otg_type`, `read_otg_csv_wrapper()` has the argument `otg_type_list` where the user can provide a vector of OTG data types to import.

Let's instead use `read_otg_csv_wrapper()` to import just our channel unit and wood data, similar to above, except now creating a list of tibbles, rather than two separate objects.

```{r read-otg-csv-wrapper-some, eval = F}
# read in some (channel unit and wood) data types at once using read_otg_csv_wrapper()
otg_raw = read_otg_csv_wrapper(path = otg_path,
                               otg_type_list = c("CU_1.csv",
                                                 "Wood_2.csv"))

```

Better yet, let's read in all OTG data types at once, which, by the way, is the default setting for `read_otg_csv_wrapper()`. Note that the function includes an argument `otg_type_names` which can be used to rename each of the tibbles during import, which is much tidier than using the file names to name each tibble (as we did above without `otg_type_names`). If `otg_type_names` is provided by the user, it **must** be the same length as `otg_type_list`.

```{r read-otg-csv-wrapper, eval = F}
# "loop over" all data types using read_otg_csv_wrapper()
otg_raw = read_otg_csv_wrapper(path = otg_path,
                               otg_type_list = c("surveyPoint_0.csv",
                                                 "CU_1.csv",
                                                 "Wood_2.csv",
                                                 "Jam_3.csv",
                                                 "Undercut_4.csv",
                                                 "Discharge_5.csv",
                                                 "DischargeMeasurements_6.csv"),
                               otg_type_names = c("survey",
                                                  "cu",
                                                  "wood",
                                                  "jam",
                                                  "undercut",
                                                  "discharge",
                                                  "discharge_measurements"))

```

Congratulations! You now (hopefully) have successfully imported your [ArcGIS Survey123](https://www.esri.com/en-us/arcgis/products/arcgis-survey123/overview) OTG data using the `read_otg_csv()` or `read_otg_csv_wrapper()` (preferred) functions. Either that, or you're muttering obscenities under your breath, in which case you're welcome to either submit an [Issue](https://github.com/BiomarkABS/DASH/issues) on the DASH website or contact the author at the e-mail address provided above.

We've included an example "raw" OTG dataset with the package called `otg_raw` which was imported using `read_otg_csv_wrapper()`. `otg_raw` includes data for just `r nrow(otg_raw$survey)` sites. Let's explore `otg_raw` a bit; you can use `$` to extract elements of a list.

```{r view-raw-otg}
# a summary of the cu data (just the first 10 columns; exclude [,1:10] to see all)
summary(otg_raw$cu[,1:10])

# look at first 5 records of wood data using head()
head(otg_raw$wood)

```

Excellent, now let's save our data to an (imaginary) directory for later use. Next, we'll perform quality control (QC) on our OTG data.

```{r save-raw-otg, eval = F}
save(otg_raw,
     file = paste0(nas_prefix, "/data/habitat/DASH/OTG/prepped/otg_raw.rda"))

```

# Initial QC

It's now time to perform QC on the imported raw data to identify errors that may have occurred in the field during data collection. The goal is to identify as many potential errors in the data as possible (but perhaps not all!) using functions provided by the [DASH](https://github.com/BiomarkABS/DASH) R package; and save or export a list of those potential errors. The identified errors can then be reviewed by field personnel or someone closely familiar with the data and remedied where possible.

Please note that during the QC process or while working with the data, you may very well identify potential errors in the data **not** identified by functions provided here. In this case, please feel free to submit an [Issue](https://github.com/BiomarkABS/DASH/issues) on the GitHub repo website and we will try to add the functionality to "flag" or identify those potential issues. We are always looking to improve the QC process.

Let's start by loading the `otg_raw` object that we previously saved, using the `load()` function:

```{r fake-load-raw-otg, eval = F}
load(paste0(nas_prefix,
            "/data/habitat/DASH/OTG/prepped/otg_raw.rda"))

```

Recognize that you don't need to load `otg_raw` if it's already in your environment. But it is good practice to save some of your data objects at particular "checkpoints" during analysis (e.g., after finishing the data import process). `load()` is then useful in case you had to step away for awhile (i.e., close your script, analysis, project) and come back later.

To QC the OTG data, DASH includes 7 QC functions as follows (including a description of the QC checks that each performs), each corresponding to an `otg_type`:

* `qc_survey()`: For `otg_type = "surveyPoint_0.csv` data.
    * Are there duplicate sites?
    * Do the latitude and longitude values fall within an expected range?
* `qc_cu()`: For `otg_type = "CU_1.csv"` data.
    * Are all channel unit types valid?
    * Are all channel unit numbers within each survey unique?
    * Are the thalweg exit depth and maximum depth values all within an expected range?
    * Does the sum of the fish cover values fall within an expected range?
    * Do the ocular substrate estimates for all slow water channel units sum to 100?
    * Do the riffles with pebble counts have all columns filled in, and if so, do values fall within an expected range?
* `qc_wood()`: For `otg_type = "Wood_2.csv"` data.
    * Are there any strange values in the **Wet?**, **Channel Forming?**, or **Ballasted?** columns?
    * Are the length and diameter measurements for any pieces of wood possibly reversed?
    * Are there any number, length, or diameter values that fall outside expected values?
* `qc_jam()`: For `otg_type = "Jam_3.csv"` data.
    * Do any of the number of pieces, length, width, or height values fall outside an expected range?
* `qc_undercut()`: For `otg_type = "Undercut_5.csv"` data.
    * Are there any strange entries in the **Location** column?
    * Are there any undercut number, length, or width values that fall outside expected values?
* `qc_disch()`: For `otg_type = "Discharge_5.csv"` data.
    * Are any channel units not numeric?
    * Are any channel units negative?
* `qc_disch_meas()`: For `otg_type = "DischargeMeasurements_6.csv"` data.
    * Do any of the width, depth, or velocity measurements fall outside of expected values?

In addition to the above, each QC function also 1) identifies whether the column names in `qc_df` match the expected defined in `get_otg_cols_specs()`, and 2) checks whether there are any `NA` or <blanks> in important columns defined using the `cols_to_check_nas` argument (where applicable).

## QC One `otg_type` at a Time

We can QC a single `otg_type` at a time using any number of the above QC functions. As an example, let's use `qc_cu()` and `qc_wood()` to QC the `otg_type = "CU_1.csv"` and `otg_type = "Wood_2.csv` data, respectively. The output from all of the QC functions has the same format and so can be easily joined, which we also demonstrate. Many of the QC functions include an argument `cols_to_check_nas` which can be used to define columns where you want to flag `NA`s or <blank> values, which can be useful for columns where you expect every record to have a value. Each QC function with the `cols_to_check_nas` argument has a default which we be used if the user does not define the argument.

```{r init-qc, eval = F}
# QC just the channel unit data using "qc_cu()", define cols_to_check_nas
init_qc_cu = qc_cu(qc_df = otg_raw$cu,
                   cols_to_check_nas = c("Channel Unit Type",
                                       "Channel Unit Number",
                                       "Channel Segment Number",
                                       "Maximum Depth (m)",
                                       "ParentGlobalID"))

# QC just the wood data using "qc_wood()", use default cols_to_check_nas
init_qc_wood = qc_wood(qc_df = otg_raw$wood)

# for example, bind the above results together
init_qc_some = init_qc_cu %>%
  add_column(source = "CU",
             .before = 0) %>%
  bind_rows(init_qc_wood %>%
              add_column(source = "Wood",
                         .before = 0))

```

## `qc_wrapper()`: QC All OTG Data At Once

For the sake of convenience, we also include a `qc_wrapper()` function that "loops over" some or all of the 7 QC functions, similar to the `read_otg_csv_wrapper()` function we used earlier during data import. In addition, `qc_wrapper()` also combines the results from each of the QC's to a single data.frame.

Similar to above, let's just QC the `otg_type = "CU_1.csv"` and `otg_type = "Wood_2.csv"` data, except using the `qc_wrapper()` function. We also demonstrate how to use `qc_wrapper()` to QC every `otg_type` at once and saving it to an object `init_qc`. The `qc_wrapper()` function will provide a series of messages while performing the QC providing periodic updates; the function includes the argument `redirect_output`, which if set to `FALSE` will write those messages to the console. However, if `redirect_output = TRUE`, those messages can be written to a file using the additional argument `redirect_output_path`.

```{r init-qc-all, eval = F}
# QC just channel unit and wood data
init_qc_some = qc_wrapper(cu_df = otg_raw$cu,
                          wood_df = otg_raw$wood,
                          redirect_output = FALSE)

# QC all OTG data types at once
init_qc = qc_wrapper(survey_df = otg_raw$survey,
                     cu_df = otg_raw$cu,
                     wood_df = otg_raw$wood,
                     jam_df = otg_raw$jam,
                     undercut_df = otg_raw$undercut,
                     discharge_df = otg_raw$discharge,
                     disch_meas_df = otg_raw$discharge_measurements,
                     redirect_output = FALSE)

```

At this point, it would be a good idea to write your initial QC results to a .csv file. I'd suggest writing the file to
the "/1_formatted_csvs/" directory containing the data that was QC'd. If you want to get fancy, you can also append today's date to the name of the file so that someone knows the date that the last QC was performed.

```{r save-qc, eval = F}
# write QC results to .csv
write_csv(init_qc,
          paste0(nas_prefix, "/data/habitat/DASH/OTG/1_formatted_csvs/init_qc.csv"))

# append today's date
write_csv(init_qc,
          paste0(nas_prefix,
                 "/data/habitat/DASH/OTG/1_formatted_csvs/init_qc_",
                 format(Sys.Date(), format = "%Y%m%d"),
                 ".csv"))

```

Nice work! You just completed an initial QC on your OTG data and exported the results to a file. Either that, or you're again muttering obscenities at the authors, at which point you're welcome to contact them at the above provided e-mail addresses or submit an [Issue](https://github.com/BiomarkABS/DASH/issues) on the repo website. Let's move onto examining those QC results, and in some cases, resolving them.

# Examine QC Results

Of course, a user could examine the initial QC results `init_qc` by opening and reviewing the .csv file written above. However, you might also find it useful to explore the QC results a little further using R. The following section is completely optional, but provides some examples and guidance about how `init_qc` could be examined in R.

## QC Review

***At this point in the workflow, someone very familiar with the data (preferably a field technician or field coordinator, secondarily a project leader) should likely intervene, review the remaining QC errors that were just written out to file, attempt to resolve those where possible, and ideally, make notes for those QC errors that can't be resolved. In addition, for the QC errors that are resolved, it is useful to provide notes on how they are resolved.*** Notes on how QC errors were resolved (or not) can be useful toward improving data validation (e.g., during field collection) ro quality control steps in the future.

Similar to above, it is generally best practice to not modify the "raw" data i.e., in our case, do not make changes to the data in the "/1_formatted_csvs/" directory. Instead, it is a good idea to make another copy of your data (I use "/2_qcd_csvs/") and attempt to resolve identified issues in that data. Doing so preserves the integrity of the raw data in "/1_formatted_csvs/". In this case, someone could then fairly easily identify any changes that were made to the data during the QC process. In addition, by documenting changes that have been made to the data, we can identify common errors and perhaps add data validation during the data collection process to prevent those errors in the future.

*Note: We have also saved a version of the data in a "/0_raw_csvs/" directory, which is actually the "rawest" form of the data from Survey123. The data in the "/0_raw_csvs/" and "/1_formatted_csvs/" directories only differ in that file formatting issues may have been identified during the data import process and resolved in the "/1_formatted_csvs/" version; however, no data has actually changed.

We can perform and initial review of our QC results using the following:

```{r examine-qc}
# use janitor::tabyl() to examine the "source" of the errors
tabyl(init_qc,
      source)

# simply print out the results
init_qc %>%
  print()

```

Here is simply an example of how one could examine a particular type of error. In this case, let's look at the "CU" data and look at those errors where `error_message` contains "Column Maximum Depth". We also join the source data (`otg_raw$cu`) back to the error message to look a little closer. The `dplyr` function `select()` can be used to choose the columns to include in your results.

The following approach can be used to examine many (if not all) of the types of errors identified during the QC process. Alternatively, one could simply examine the QC results file we wrote above.

```{r max-depth-errors}
# examine channel unit errors where the error_message contains "Column Maximum Depth"
init_qc %>%
  filter(source == "CU") %>%
  filter(grepl("Column Maximum Depth", error_message)) %>%
  left_join(otg_raw$cu) %>%
  select(source:error_message, `Channel Unit Type`, `Maximum Depth (m)`)
  
```

During our initial review of DASH data, we commonly identified the following two "types" of errors: 1) the sum of ocular substrate estimates don't sum to 100 due to a math error in the field and/or 2) the sum of fish cover estimates do not fall within an expected range (note: the sum of fish cover estimates **can** sum to greater than 100). We've included two functions within the package `rescale_values()` and `fix_fish_cover()` that can be used to resolve a large number of the identified errors, respectively. We cover those functions in the following [section][Re-import QC'd Data and Resolve Common Errors].

Examine the ocular substrate errors:

```{r ocular-ests-errors}
# ocular estimate errors where sum_ocular does not equal 100
init_qc %>%
  filter(source == "CU") %>%
  filter(grepl("Ocular estimates", error_message)) %>%
  left_join(otg_raw$cu) %>%
  select(source:GlobalID,
         `Channel Unit Type`,
         `Sand/Fines 2mm`:`Boulder 256mm`) %>%
  mutate(sum_ocular = rowSums(.[5:8], na.rm = T))
  
```

Examine the fish cover errors:

```{r fish-cover-errors}
# the sum of fish cover values fall outside the expected range
init_qc %>%
  filter(source == "CU") %>%
  filter(grepl("Cover values", error_message)) %>%
  left_join(otg_raw$cu) %>%
  select(source:GlobalID,
         `Overhanging Cover`:`Total No Cover`) %>%
  mutate(sum_cover = rowSums(.[4:8], na.rm = T))
  
```

Here's one last example for exploring a somewhat common error. In this case, let's look at all errors in the undercut data `filter(source == "Undercut")`, join the source data `left_join(otg_raw$undercut)`, and then reduce the number of columns included in the results ``select(path_nm:error_message, `Undercut Number`:`Width 75% (m)`)``. Note in this case we need to place backticks around the variable (column) names "Undercut Number" and "Width 75% (m)" because they contain invalid or special character (e.g., <space>, %, ()).

```{r undercut-errors}
# undercut errors; note many of these are flagged bc of a strange "Location" value "Island" which perhaps should be an acceptable value
init_qc %>%
  filter(source == "Undercut") %>%
  left_join(otg_raw$undercut) %>%
  select(path_nm:error_message,
         `Undercut Number`:`Width 75% (m)`)
  
```

# Re-import QC'd Data and Resolve Common Errors

```{r re-import-qcd-otg, eval = F}
# As an example, if the Survey123 site folders were in a "2_qcd_csvs" folder on your Desktop
otg_path = "C:/Users/username/Desktop/2_qcd_csvs/"

# For internal Biomark folks, set nas_prefix based on your operating system and then set otg_path to the NAS
if(.Platform$OS.type != 'unix') {
  nas_prefix = "S:"
} else if(.Platform$OS.type == 'unix') {
  nas_prefix = "~/../../Volumes/ABS"
}

# a theoretical path on the NAS
otg_path = paste0(nas_prefix, "/data/habitat/DASH/OTG/2_qcd_csvs/")

# "loop over" all data types using read_otg_csv_wrapper()
otg_qcd = read_otg_csv_wrapper(path = otg_path,
                               otg_type_list = c("surveyPoint_0.csv",
                                                 "CU_1.csv",
                                                 "Wood_2.csv",
                                                 "Jam_3.csv",
                                                 "Undercut_4.csv",
                                                 "Discharge_5.csv",
                                                 "DischargeMeasurements_6.csv"),
                               otg_type_names = c("survey",
                                                  "cu",
                                                  "wood",
                                                  "jam",
                                                  "undercut",
                                                  "discharge",
                                                  "discharge_measurements"))

```

```{r fix-oc-ests, eval = F}
# ocular substrate fixes
otg_qcd$cu = rescale_values(data_df = otg_qcd$cu,
                            col_names = c("Sand/Fines 2mm",
                                          "Gravel 2-64mm",
                                          "Cobble 64-256mm",
                                          "Boulder 256mm"),
                            min_value = 90,
                            max_value = 110,
                            sum_to = 100)

```

```{r fix-fish-cover, eval = F}
# fish cover fixes
otg_qcd$cu = fix_fish_cover(cu_df = otg_qcd$cu,
                            cover_cols = c("Overhanging Cover",
                                           "Aquatic Vegetation",
                                           "Woody Debris Cover",
                                           "Artificial Cover",
                                           "Total No Cover"))

```

```{r save-qcd-otg, eval = F}
save(otg,
     file = paste0(nas_prefix, "/data/habitat/DASH/OTG/prepped/otg.rda"))

```

# Final QC

```{r fake-load-qcd-otg, eval = F}
load(paste0(nas_prefix,
            "/data/habitat/DASH/OTG/prepped/otg.rda"))

```

```{r final-qc, eval = F}
qc_final = qc_wrapper(survey_df = otg$survey,
                      cu_df = otg$cu,
                      wood_df = otg$wood,
                      jam_df = otg$jam,
                      undercut_df = otg$undercut,
                      discharge_df = otg$discharge,
                      disch_meas_df = otg$discharge_measurements)

```

```{r save-final-qc, eval = F}
# set path to write to
qc_final_path = paste0(nas_prefix, "/data/habitat/DASH/OTG/2_qcd_csvs/")

# write QC results to .csv
write_csv(qc_final,
          paste0(qc_final_path, "qc_final.csv"))

# append today's date, if desired
write_csv(qc_final,
          paste0(qc_final_path, 
                 "qc_final_",
                 format(Sys.Date(), format = "%Y%m%d"),
                 ".csv"))

# can also be useful to save as a .rds, more on this later...
save(qc_final, file = paste0(qc_final_path, "qc_final.rds"))

```

# Data Wrangling and Export

```{r clean-cu, eval = F}
# clean channel unit data and join site info to it
cu_cu = rollup_cu(cu_df = otg$cu,
                  survey_df = otg$survey)

```

```{r rollup, eval = F}
# summarize wood data at cu scale
cu_wood = rollup_cu_wood(wood_df = otg$wood)

# summarize jam data at cu scale
cu_jam = rollup_cu_jam(jam_df = otg_jam)

# summarize undercut data at cu scale
cu_undercut = rollup_cu_undercut(undercut_df = otg$undercut)

```

```{r rollup-discharge, eval = F}
# rolling up discharge is a little trickier
cu_discharge = rollup_cu_discharge(discharge_df = otg$discharge,
                                   discharge_meas_df = otg$discharge_measurements) %>%
  # Add the below to the rollup_cu_discharge() function?
  left_join(otg$survey %>%
              select(global_id,
                     site_name),
            by = c("parent_global_id" = "global_id")) %>%
  mutate(cu_id = paste(site_name,
                       "01",
                       str_pad(discharge_location_bos_tos_cu_number, 3, pad = "0"),
                       sep = "_")) %>%
  select(site_name,
         cu_id,
         discharge_cms,
         discharge_location_bos_tos_cu_number,
         everything()) %>%
  left_join(cu_cu %>%
              select(parent_global_id,
                     cu_id,
                     channel_unit_number) %>%
              group_by(parent_global_id) %>%
              summarise(min_cu = min(channel_unit_number),
                        max_cu = max(channel_unit_number))) %>%
  select(cu_id,
         discharge_cms)

```

```{r join-cu-data, eval = F}
# join everything at the channel unit scale
dash_otg_cu = list(cu_cu %>%
                     rename(site_id = global_id),
                   cu_wood,
                   cu_jam,
                   cu_undercut) %>%
  purrr::reduce(left_join,
                by = "parent_global_id") %>%
  left_join(cu_discharge)

```

```{r export-otg, eval = F}
# write the final product
save(dash_otg_cu,
     file = paste0(nas_prefix, "data/habitat/DASH/OTG/prepped/dash_otg_cu.rda"))

write_csv(dash_otg_cu,
          path = paste0(nas_prefix, "data/habitat/DASH/OTG/prepped/dash_otg_cu.csv"))

```

#### END IMPORT AND QC VIGNETTE

*Holy Guacamole! That deserves a beer!*
